{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import model\n",
    "import datasets\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_file(fp, classes, img_per_class):\n",
    "    '''\n",
    "    creates valid_list.txt and outputs a new train_list.txt as well\n",
    "    fp --> file path for original train_list\n",
    "    img_per_class --> how many images we want per class\n",
    "    '''\n",
    "    num_classes = np.arange(classes)\n",
    "\n",
    "    valid_1 = []\n",
    "    train_1 = []\n",
    "    valid_2 = []\n",
    "    train_2 = []\n",
    "\n",
    "    with open(fp) as f:\n",
    "        all_fps = f.readlines()\n",
    "\n",
    "    all_fps = np.array([x.split() for x in all_fps])\n",
    "\n",
    "\n",
    "    for clss in num_classes:\n",
    "        filtered = all_fps[all_fps[:, 1] == str(clss)]\n",
    "\n",
    "        choice = np.random.choice(len(filtered), 2 * img_per_class, replace=False)\n",
    "        print(choice)\n",
    "        for element in np.arange(len(filtered)):\n",
    "            # append to valid\n",
    "            if element in choice[:3]:\n",
    "                valid_1.append(filtered[element])\n",
    "            else:\n",
    "                train_1.append(filtered[element])\n",
    "        for element in np.arange(len(filtered)):\n",
    "            # append to valid\n",
    "            if element in choice[3:]:\n",
    "                valid_2.append(filtered[element])\n",
    "            else:\n",
    "                train_2.append(filtered[element])\n",
    "\n",
    "    np.savetxt(\"birds_dataset/processed_train_1.txt\", train_1, fmt=\"%s\")\n",
    "    np.savetxt(\"birds_dataset/processed_val_1.txt\", valid_1, fmt=\"%s\")\n",
    "    np.savetxt(\"birds_dataset/processed_train_2.txt\", train_2, fmt=\"%s\")\n",
    "    np.savetxt(\"birds_dataset/processed_val_2.txt\", valid_2, fmt=\"%s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('Create Validation dataset')\n",
    "    # get 2 distinct train/val splits by running this twice with random commented out\n",
    "    # check there are no similarities with: grep -Fxf processed_val_1.txt processed_val.txt\n",
    "    valid_file('./birds_dataset/train_list.txt', 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        # datasets \n",
    "        self.train_dataset_1 = None\n",
    "        self.valid_dataset_1 = None\n",
    "        self.train_dataset_2 = None\n",
    "        self.valid_dataset_2 = None\n",
    "        # dataloaders for 2 holdouts\n",
    "        self.train_dataloader_1 = None\n",
    "        self.val_dataloader_1 = None\n",
    "        self.train_dataloader_2 = None\n",
    "        self.val_dataloader_2 = None\n",
    "        # test sets\n",
    "        self.test_dataset = None\n",
    "        self.test_dataloader = None\n",
    "\n",
    "        self.classes = 20\n",
    "\n",
    "        self.nn_model = None\n",
    "\n",
    "\n",
    "    def loaddata(self):\n",
    "        # load your dataset and dataloader\n",
    "        # feel free to change header of bird_dataset class\n",
    "        root = 'birds_dataset/'\n",
    "        # self.train_dataset = datasets.bird_dataset(root,'processed_train.txt')\n",
    "        self.train_dataset_1 = datasets.bird_dataset(root, 'processed_train_1.txt')\n",
    "        self.valid_dataset_1 = datasets.bird_dataset(root, 'processed_val_1.txt')\n",
    "\n",
    "        self.train_dataset_2 = datasets.bird_dataset(root, 'processed_train_2.txt')\n",
    "        self.valid_dataset_2 = datasets.bird_dataset(root, 'processed_val_2.txt')\n",
    "\n",
    "        self.test_dataset = datasets.bird_dataset(root, 'test_list.txt')\n",
    "\n",
    "\n",
    "        # Fill in optional arguments to the dataloader as you need it\n",
    "        self.train_dataloader_1 = DataLoader(dataset=self.train_dataset_1, batch_size=32,\n",
    "                                    shuffle=True, num_workers=2, pin_memory=True)\n",
    "        self.val_dataloader_1 = DataLoader(dataset=self.valid_dataset_1, batch_size=32,\n",
    "                                    shuffle=True, num_workers=2, pin_memory=True)\n",
    "        self.train_dataloader_2 = DataLoader(dataset=self.train_dataset_2, batch_size=32,\n",
    "                                             shuffle=True, num_workers=2, pin_memory=True)\n",
    "        self.val_dataloader_2 = DataLoader(dataset=self.valid_dataset_2, batch_size=32,\n",
    "                                           shuffle=True, num_workers=2, pin_memory=True)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=32,\n",
    "                                    shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    def init_vgg(self, freeze=False):\n",
    "        vgg = torchvision.models.vgg16_bn(pretrained=True)\n",
    "        # freeze layers\n",
    "        if freeze:\n",
    "            for param in vgg.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Modify last layer\n",
    "        vgg.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=self.classes, bias=True))\n",
    "#         print(vgg)\n",
    "\n",
    "        self.nn_model = vgg.cuda()\n",
    "\n",
    "    def init_resnet(self, freeze=False):\n",
    "        resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        # freeze layers\n",
    "        if freeze:\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Modify last layer\n",
    "        num_ftrs = resnet.fc.in_features\n",
    "        resnet.fc = nn.Linear(num_ftrs, self.classes)\n",
    "        resnet.fc.requires_grad_ = True\n",
    "\n",
    "        self.nn_model = resnet.cuda()\n",
    "\n",
    "    def init_model(self):\n",
    "        # Create NN model object\n",
    "        nn_model = model.baseline_Net(classes=20)\n",
    "        self.nn_model = nn_model.cuda()\n",
    "        # Initialize weights\n",
    "        def weights_init(m):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "        self.nn_model.apply(weights_init)\n",
    "        print(self.nn_model)\n",
    "\n",
    "    def init_custom(self):\n",
    "        # Create NN model object\n",
    "        nn_model = model.custom_Net(classes=20)\n",
    "        self.nn_model = nn_model.cuda()\n",
    "        # Initialize weights\n",
    "        def weights_init(m):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "        self.nn_model.apply(weights_init)\n",
    "        print(self.nn_model)\n",
    "\n",
    "\n",
    "    def train(self, train_dataloader, val_dataloader, epoch):\n",
    "        # 4a: Create loss functions, optimizers\n",
    "        # For baseline model use this\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=0.0001)\n",
    "        \n",
    "        decayRate = 0.96\n",
    "        #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=20, gamma=decayRate)\n",
    "        \n",
    "\n",
    "        train_loss, train_accuracy = [], []\n",
    "        val_loss, val_accuracy = [], []\n",
    "        best_val_loss = 100\n",
    "\n",
    "        # 4a: train a baseline model\n",
    "        # For each epoch iterate over your dataloaders/datasets, pass it to your NN model, get output,\n",
    "        #    calculate loss and backpropagate using optimizer\n",
    "        for epoch in tqdm(range(epoch)):\n",
    "            train_epoch_loss, train_epoch_accuracy = self.fit(self.nn_model, train_dataloader,\n",
    "                                                        optimizer, criterion)\n",
    "            val_epoch_loss, val_epoch_accuracy = self.validate(self.nn_model, val_dataloader, criterion)\n",
    "\n",
    "            # record acc and losses\n",
    "            train_loss.append(train_epoch_loss)\n",
    "            train_accuracy.append(train_epoch_accuracy)\n",
    "            val_loss.append(val_epoch_loss)\n",
    "            val_accuracy.append(val_epoch_accuracy)\n",
    "            tqdm.write('Train Loss: {}, Train Acc: {}'.format(\n",
    "                train_epoch_loss, train_epoch_accuracy))\n",
    "            tqdm.write('Val Loss: {}, Val Acc: {}'.format(\n",
    "                val_epoch_loss, val_epoch_accuracy))\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "        return train_loss, train_accuracy, val_loss, val_accuracy\n",
    "\n",
    "    def test(self, model_Net):\n",
    "        # test the network\n",
    "\n",
    "        # load your dataset and dataloader\n",
    "        # feel free to change header of bird_dataset class\n",
    "\n",
    "        if model_Net == \"baseline\":\n",
    "            testmodel = model.baseline_Net(classes=20)\n",
    "            testmodel.load_state_dict(torch.load(PATH + \"best.pth\"))\n",
    "            testmodel.to(device)\n",
    "        else:\n",
    "            testmodel = model.custom_Net(classes=20)\n",
    "            testmodel.load_state_dict(torch.load(PATH + \"custom.pth\"))\n",
    "            testmodel.to(device)\n",
    "            \n",
    "        accuracy = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(self.test_dataloader):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs_batch, labels_batch = data_batch\n",
    "                inputs_batch = inputs_batch.cuda()\n",
    "                labels_batch = labels_batch.cuda().long()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = testmodel(inputs_batch)\n",
    "\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                accuracy += float((preds == labels_batch).sum()) / float(labels_batch.shape[0])\n",
    "\n",
    "            \n",
    "            accuracy = accuracy/len(self.test_dataloader)\n",
    "        \n",
    "        print('Test Acc: ', accuracy)\n",
    "\n",
    "\n",
    "    def fit(self, model, dataloader, optimizer, criterion):\n",
    "        '''\n",
    "        function fits the model\n",
    "        returns the training loss and train_accuracy\n",
    "        '''\n",
    "        running_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for i_batch, data_batch in enumerate(dataloader):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs_batch, labels_batch = data_batch\n",
    "            inputs_batch = inputs_batch.cuda()\n",
    "            labels_batch = labels_batch.cuda().long()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "            accuracy += float((preds == labels_batch).sum()) / float(labels_batch.shape[0])\n",
    "\n",
    "\n",
    "            # backwards propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss = running_loss/len(dataloader)\n",
    "        acc = accuracy/len(dataloader)\n",
    "  \n",
    "        return running_loss, acc\n",
    "\n",
    "    def validate(self, model, dataloader, criterion):\n",
    "        '''\n",
    "        runs model on the validation set\n",
    "        '''\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(dataloader):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs_batch, labels_batch = data_batch\n",
    "                inputs_batch = inputs_batch.cuda()\n",
    "                labels_batch = labels_batch.cuda().long()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs_batch)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "\n",
    "                running_loss += loss.item() \n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                accuracy += float((preds == labels_batch).sum()) / float(labels_batch.shape[0])\n",
    "\n",
    "            \n",
    "            running_loss = running_loss/len(dataloader)\n",
    "            accuracy = accuracy/len(dataloader)\n",
    "\n",
    "        return running_loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "    def kfolds(self, init_model, name, freeze=None):\n",
    "        # fold 1\n",
    "        if freeze is not None:\n",
    "            init_model(freeze)\n",
    "        else:\n",
    "            init_model()\n",
    "        train_loss1, train_accuracy1, val_loss1, val_accuracy1 = self.train(self.train_dataloader_1, self.val_dataloader_1)\n",
    "\n",
    "        # fold 2\n",
    "        if freeze is not None:\n",
    "            init_model(freeze)\n",
    "        else:\n",
    "            init_model()\n",
    "        train_loss2, train_accuracy2, val_loss2, val_accuracy2 = self.train(self.train_dataloader_2, self.val_dataloader_2)\n",
    "\n",
    "        train_loss = [((train_loss1[i] + train_loss2[i]) / 2) for i in range(len(train_loss1))]\n",
    "        train_accuracy =  [((train_accuracy1[i] + train_accuracy2[i]) / 2) for i in range(len(train_accuracy1))]\n",
    "        val_loss = [((val_loss1[i] + val_loss2[i]) / 2) for i in range(len(val_loss2))]\n",
    "        val_accuracy = [((val_accuracy1[i] + val_accuracy2[i]) / 2) for i in range(len(val_accuracy1))]\n",
    "\n",
    "        PATH = './saved_models/'\n",
    "        torch.save(trainer.nn_model.state_dict(), PATH + name + \".pth\")\n",
    "        return train_loss, train_accuracy, val_loss, val_accuracy\n",
    "\n",
    "    def graph(self, train_loss, train_accuracy, val_loss, val_accuracy):\n",
    "        plt.plot(train_accuracy)\n",
    "        plt.plot(val_accuracy)\n",
    "        plt.title('Training vs. Validation Accuarcy')\n",
    "        plt.legend(['train', 'val'])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(train_loss)\n",
    "        plt.plot(val_loss)\n",
    "        plt.title('Training vs. Validation Loss')\n",
    "        plt.legend(['train', 'val'])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Make sure to use the GPU. The following line is just a check to see if GPU is availables\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.set_device(device)\n",
    "    print(device)\n",
    "\n",
    "    trainer = Trainer()\n",
    "    trainer.loaddata()\n",
    "\n",
    "    # show example\n",
    "    print('Train', len(trainer.train_dataset_1), ' Valid:', len(\n",
    "        trainer.valid_dataset_1), ' Test:', len(trainer.test_dataset))\n",
    "    img, l = trainer.train_dataset_1[0]\n",
    "    print(img.shape)\n",
    "    print(l, type(l))\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)), interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "    # baseline net\n",
    "    #trainer.init_model()\n",
    "    #train_loss, train_accuracy, val_loss, val_accuracy = trainer.train(trainer.train_dataloader_1, trainer.val_dataloader_1)\n",
    "    #trainer.graph(train_loss, train_accuracy, val_loss, val_accuracy)\n",
    "    #PATH = './saved_models/'\n",
    "    #torch.save(trainer.nn_model.state_dict(), PATH + \"best.pth\")\n",
    "    \n",
    "    # test custom\n",
    "    #trainer.test(\"baseline\")\n",
    "    \n",
    "    # custom net\n",
    "    trainer.init_custom()\n",
    "    train_loss, train_accuracy, val_loss, val_accuracy = trainer.train(trainer.train_dataloader_1, trainer.val_dataloader_1, 75)\n",
    "    trainer.graph(train_loss, train_accuracy, val_loss, val_accuracy)\n",
    "    PATH = './saved_models/'\n",
    "    torch.save(trainer.nn_model.state_dict(), PATH + \"custom.pth\")\n",
    "    \n",
    "    # test custom\n",
    "    trainer.test(\"custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.0001\n",
    "# epoch 50: 0.4065632832080201\n",
    "# epoch 75: 0.4197994987468672\n",
    "\n",
    "# lr = 0.001\n",
    "# epoch 75: 0.39442355889724307\n",
    "-----\n",
    "## exp LR\n",
    "# w/ scheduler\n",
    "# epoch 75: 0.39262218045112784\n",
    "\n",
    "# w/ scheduler, leaky relu 0.001\n",
    "# epoch 75: 0.41220238095238093\n",
    "\n",
    "# w/ scheduler, leaky relu 0.0001\n",
    "# epoch 75: 0.39747807017543857\n",
    "\n",
    "# w/ scheduler, leaky relu 0.01\n",
    "# epoch 75: doesnt work, acc only 0.1 no save\n",
    "------\n",
    "## stepLR\n",
    "# w/ scheduler, leaky relu 0.0001\n",
    "# epoch 75: is bad too 0.18 no save\n",
    "\n",
    "# 0.001, 10, 0.01\n",
    "# epoch 75: 0.19415726817042608 no save\n",
    "\n",
    "# 0.001, 20, 0.1\n",
    "# epoch 75: 0.32346491228070173\n",
    "\n",
    "# 0.001, 20, 0.3\n",
    "# epoch 75: 0.3620770676691729\n",
    "\n",
    "# 0.001, 20, 0.6\n",
    "# epoch 75: 0.38024749373433586\n",
    "\n",
    "# 0.001, 30, 0.6\n",
    "# epoch 75: 0.34234022556390975\n",
    "\n",
    "# 0.001, 20, 0.8\n",
    "# epoch 75: 0.39262218045112784\n",
    "\n",
    "# 0.001, 25, 0.96\n",
    "# epoch 75: 0.3925438596491228\n",
    "\n",
    "# 0.0001, 20, 0.96\n",
    "# epoch 75: 0.4230889724310777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# w = custom.b1[0].weight.data.cpu()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, filter in enumerate(w):\n",
    "#     plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "#     plt.imshow(filter[0, :, :].detach())\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig('images/custom_firstlayer.png')\n",
    "# # plt.show()\n",
    "# plt.clf()\n",
    "\n",
    "# model_children = list(resnet.children())\n",
    "# w = model_children[0].weight\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, filter1 in enumerate(w):\n",
    "#     plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "#     plt.imshow(filter1[0, :, :].detach().cpu())\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig('images/resnet_firstlayer.png')\n",
    "# # plt.show()\n",
    "# plt.clf()\n",
    "\n",
    "\n",
    "model_children = list(vgg.children())\n",
    "w = model_children[0][0].weight\n",
    "plt.figure(figsize=(20, 17))\n",
    "for i, filter in enumerate(w):\n",
    "    plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "    plt.imshow(filter[0, :, :].detach().cpu())\n",
    "    plt.axis('off')\n",
    "    plt.savefig('images/vgg_firstlayer.png')\n",
    "# plt.show()\n",
    "plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer()\n",
    "# trainer.loaddata()\n",
    "# trainer.init_vgg()\n",
    "# vgg = trainer.nn_model\n",
    "\n",
    "\n",
    "# PATH = './saved_models/'\n",
    "# custom = model.custom_Net(classes=20).cuda()\n",
    "# custom.load_state_dict(torch.load(PATH + \"custom.pth\"))\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.loaddata()\n",
    "trainer.init_resnet()\n",
    "resnet = trainer.nn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# first layer\n",
    "img, l = trainer.test_dataset[0]\n",
    "plt.imshow(np.transpose(img, (1, 2, 0)), interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************************************************\n",
    "# PLOTS WERE CREATED SEPARATELY TO PREVENT THE KERNEL FROM DYING!!!!!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "# model = custom\n",
    "# # custom model: initial layer\n",
    "# model.b1.register_forward_hook(get_activation('Conv2d'))\n",
    "# data, _ = trainer.test_dataset[0]\n",
    "# data.unsqueeze_(0)\n",
    "# output = model(data.cuda())\n",
    "# act = activation['Conv2d'].squeeze()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, feature in enumerate(act):\n",
    "#     plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(feature.cpu())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig('images/custom_initial.png')\n",
    "# plt.show()\n",
    "# plt.clf\n",
    "\n",
    "\n",
    "# # custom model: middle layer\n",
    "# model.b6.register_forward_hook(get_activation('Conv2d'))\n",
    "# data, _ = trainer.test_dataset[0]\n",
    "# data.unsqueeze_(0)\n",
    "# output = model(data.cuda())\n",
    "# act = activation['Conv2d'].squeeze()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, feature in enumerate(act):\n",
    "#     plt.subplot(16, 16, i + 1)\n",
    "#     plt.imshow(feature.cpu())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig('images/custom_middle.png')\n",
    "# plt.show()\n",
    "# plt.clf\n",
    "\n",
    "\n",
    "# custom model: final layer\n",
    "# model.b6.register_forward_hook(get_activation('conv2d'))\n",
    "# data, _ = trainer.test_dataset[0]\n",
    "# data.unsqueeze_(0)\n",
    "# output = model(data.cuda())\n",
    "# act = activation['conv2d'].squeeze()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, feature in enumerate(act):\n",
    "#     plt.subplot(16, 16, i + 1)\n",
    "#     plt.imshow(feature.cpu())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig('images/custom_end.png')\n",
    "# plt.show()\n",
    "# plt.clf\n",
    "\n",
    "################################################################################\n",
    "# model = vgg\n",
    "\n",
    "# model_children = list(model.children())\n",
    "\n",
    "# model_children[0][0].register_forward_hook(get_activation('Conv2d'))\n",
    "# data, _ = trainer.test_dataset[0]\n",
    "# data.unsqueeze_(0)\n",
    "# output = model(data.cuda())\n",
    "# act = activation['Conv2d'].squeeze()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, feature in enumerate(act):\n",
    "#     plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(feature.cpu())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig('images/vgg_initial.png')\n",
    "# plt.show()\n",
    "# plt.clf\n",
    "\n",
    "# model_children[0][20].register_forward_hook(get_activation('Conv2d'))\n",
    "# data, _ = trainer.test_dataset[0]\n",
    "# data.unsqueeze_(0)\n",
    "# output = model(data.cuda())\n",
    "# act = activation['Conv2d'].squeeze()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, feature in enumerate(act):\n",
    "#     plt.subplot(16, 16, i + 1)\n",
    "#     plt.imshow(feature.cpu())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig('images/vgg_middle.png')\n",
    "# plt.show()\n",
    "# plt.clf\n",
    "\n",
    "\n",
    "# model_children[0][40].register_forward_hook(get_activation('Conv2d'))\n",
    "# data, _ = trainer.test_dataset[0]\n",
    "# data.unsqueeze_(0)\n",
    "# output = model(data.cuda())\n",
    "# act = activation['Conv2d'].squeeze()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, feature in enumerate(act):\n",
    "#     plt.subplot(16, 16, i + 1)\n",
    "#     plt.imshow(feature.cpu())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig('images/vgg_end.png')\n",
    "# plt.show()\n",
    "# plt.clf\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "model = resnet\n",
    "model_children = list(model.children())\n",
    "\n",
    "\n",
    "# model_children[0].register_forward_hook(get_activation('Conv2d'))\n",
    "# data, _ = trainer.test_dataset[0]\n",
    "# data.unsqueeze_(0)\n",
    "# output = model(data.cuda())\n",
    "# act = activation['Conv2d'].squeeze()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, feature in enumerate(act):\n",
    "#     plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(feature.cpu())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig('images/resnet_initial.png')\n",
    "# plt.show()\n",
    "# plt.clf\n",
    "\n",
    "\n",
    "# model_children[5][0].register_forward_hook(get_activation('Conv2d'))\n",
    "# data, _ = trainer.test_dataset[0]\n",
    "# data.unsqueeze_(0)\n",
    "# output = model(data.cuda())\n",
    "# act = activation['Conv2d'].squeeze()\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# for i, feature in enumerate(act):\n",
    "#     plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(feature.cpu())\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig('images/resnet_middle.png')\n",
    "# plt.show()\n",
    "# plt.clf\n",
    "\n",
    "\n",
    "\n",
    "model_children[7][0].register_forward_hook(get_activation('Conv2d'))\n",
    "data, _ = trainer.test_dataset[0]\n",
    "data.unsqueeze_(0)\n",
    "output = model(data.cuda())\n",
    "act = activation['Conv2d'].squeeze()\n",
    "plt.figure(figsize=(20, 17))\n",
    "for i, feature in enumerate(act):\n",
    "    plt.subplot(23, 23, i + 1)\n",
    "    plt.imshow(feature.cpu())\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('images/resnet_end.png')\n",
    "plt.show()\n",
    "plt.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
